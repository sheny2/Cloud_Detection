---
title: "STA 521 Project 2 Cloud Data Report"
author: "Yicheng Shen (yicheng.shen@duke.edu) & Yunhong Bao (yunhong.bao@duke.edu)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
    - \setlength{\parskip}{0em}
    - \setlength{\parindent}{2em}
    - \usepackage{indentfirst}
    - \usepackage{float}
output: 
  pdf_document: 
    extra_dependencies: ["float"]
    number_sections: true
bibliography: cloud.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = TRUE, warning=F)
library(mosaic)
library(caret)
library(gridExtra)
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(fig.pos = "H")

image_1 <- read.table("image_data/imagem1.txt")
image_2 <- read.table("image_data/imagem2.txt")
image_3 <- read.table("image_data/imagem3.txt")
var_name <- c("Y_coord", "X_coord", "Cloud", "NDAI", "SD", "CORR", "DF","CF","BF","AF","AN")
colnames(image_1) = colnames(image_2) = colnames(image_3) <- var_name
image_1$Cloud <- factor(image_1$Cloud)
image_2$Cloud <- factor(image_2$Cloud)
image_3$Cloud <- factor(image_3$Cloud)
```


\section{Data Collection and Exploration}


\subsection{Background \& Motivation}

(a) Write a half-page summary of the paper, including at least the purpose of the study, the data, the collection method, its conclusions and potential impact.


With the global climates getting increasingly extreme, humans are making the best use of sciences and technologies to understand the environment, especially in the Arctic. 
The detection of clouds in satellite images has become an important task, as cloud coverage is closely related to the surface air temperatures and atmospheric carbon dioxide levels. Yet it is a challenging problem since clouds are similar on snow- and ice-covered surface
In this study, we are going to examine various classification methods and build reliable models that distinguish the presence of cloud from Arctic satellite images using available features. 

The data is obtained from a study by @yu2008. This team of researchers collected the data via the camera of Multiangle Imaging SpectroRadiometer (MISR) launched by the NASA. 


(b) Summarize the data, i.e., % of pixels for the different classes. Plot well-labeled beautiful maps using x, y coordinates the expert labels with color of the region based on the expert labels. Do you observe some trend/pattern? Is an i.i.d. assumption for the samples justified for this dataset?

The three images contain 115110, 115229 and 115217 pixels respectively. However, not all pixels are labelled with confident experts' classification. As shown in in Figure \ref{fig:map3}, there are considerable portions of images not labelled. After removing unlabelled pixels, we have 82148, 70917 and 54996 pixels in each image, with available information. 

```{r show map, fig.cap="\\label{fig:map3}Maps of three images with expert labels. White represents high confidence cloudy; gray, high confidence clear; and black, unlabeled pixels.",fig.width = 12, fig.height = 4, out.width="85%"}
Palette_1 <- c("gray","black", "white")

map_1 <- image_1 %>%
  ggplot() + geom_point(aes(x = X_coord, y = Y_coord, color = Cloud), size = 0.5) +
  theme_map() + theme(legend.position = "none") +
  labs(title = "Image 1", x = "X Coordinate", y = "Y Coordinate") +
  scale_colour_manual(values = Palette_1)

map_2 <- image_2 %>%
  ggplot() + geom_point(aes(x = X_coord, y = Y_coord, color = Cloud), size =  0.5) +
  theme_map() + theme(legend.position = "none") +
  labs(title = "Image 2", x = "X Coordinate", y = "Y Coordinate") +
  scale_colour_manual(values = Palette_1) 

map_3 <- image_3 %>%
  ggplot() + geom_point(aes(x = X_coord, y = Y_coord, color = Cloud), size = 0.5) +
  theme_map() + theme(legend.position = "none") +
  labs(title = "Image 3", x = "X Coordinate", y = "Y Coordinate") +
  scale_colour_manual(values = Palette_1)

grid.arrange(arrangeGrob(map_1 + ggeasy::easy_center_title() + theme(legend.position="none"),
                         map_2 + ggeasy::easy_center_title() + theme(legend.position="none"),
                         map_3 + ggeasy::easy_center_title() + theme(legend.position="none"),
                         nrow = 1))
```

In specific, each pixel has eight features. The first three are physically useful features: for characterizing the scattering properties of ice- and snow-covered surfaces the correlation (CORR) of MISR images of the same scene from different MISR viewing directions, the standard deviation (SDAn) of MISR nadir camera pixel values across a scene, and a normalized difference angular index (NDAI) that characterizes the changes in a scene with changes in the MISR view direction.

The latter five are five view zenith angles of the cameras. $70.5^{\circ}$ (Df), $60.0^{\circ}$ (Cf), $45.6^{\circ}$ (BF), and $26.1^{\circ}$ (Af) in the forward direction and $0.0^{\circ}$ (An) in the nadir direction. 


(c) Perform a visual and quantitative EDA of the dataset, e.g., summarizing (i) pair- wise relationship between the features themselves and (ii) the relationship between the expert labels with the individual features. Do you notice differences between the two classes (cloud, no cloud) based on the radiance or other features (CORR, NDAI, SD)?



\section{Preparation}

(a) (Data Split) Split the entire data (imagem1.txt, imagem2.txt, imagem3.txt) into three sets: training, validation and test. Think carefully about how to split the data. Suggest at least two non-trivial different ways of splitting the data which takes into account that the data is not i.i.d.

(b) (Baseline) Report the accuracy of a trivial classifier which sets all labels to -1 (cloud-free) on the validation set and on the test set. In what scenarios will such a classifier have high average accuracy? Hint: Such a step provides a baseline to ensure that the classification problems at hand is not trivial.

(c) (First order importance) Assuming the expert labels as the truth, and without using fancy classification methods, suggest three of the “best” features, using quantitative and visual justification. Define your “best” feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

(d) Write a generic cross validation (CV) function CVmaster in R that takes a generic classifier, training features, training labels, number of folds K and a loss function (at least classification accuracy should be there) as inputs and outputs the K-fold CV loss on the training set. Please remember to put it in your github folder in Section 5.


\section{Modeling}

(a) Try several classification methods and assess their fit using cross-validation (CV). Provide a commentary on the assumptions for the methods you tried and if they are satisfied in this case. Since CV does not have a validation set, you can merge your training and validation set to fit your CV model. Report the accuracies across folds (and not just the average across folds) and the test accuracy. CV-results for both the ways of creating folds (as answered in part 2(a)) should be reported. Provide a brief commentary on the results. Make sure you honestly mention all the classification methods you have tried.

(b) Use ROC curves to compare the different methods. Choose a cutoff value and highlight it on the ROC curve. Explain your choice of the cutoff value.

(c) (Bonus) Assess the fit using other relevant metrics. Use quantitative measures and show clean and interpretable figures!


\section{Diagnostics}

(a) Do an in-depth analysis of a good classification model of your choice by showing some diagnostic plots or information related to convergence or parameter estimation.


(b) For your best classification model(s), do you notice any patterns in the misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in specific ranges of feature values?

(c) Based on parts 4(a) and 4(b), can you think of a better classifier? How well do you think your model will work on future data without expert labels?

(d) Do your results in parts 4(a) and 4(b) change as you modify the way of splitting the data?

(e) Write a paragraph for your conclusion.



\section{Reproducibility}

In addition to a writeup of the above results, please submit a zip file containing everything necessary to reproduce your writeup to Gradescope “PROJ2 code”. Specifically, imagine that at some point an error is discovered in the three image files, and a future researcher wants to check whether your results hold up with the new, corrected image files. This researcher should be able to easily re-run all your code and produce all your figures and tables. This zip file should contain:


(i) the raw Latex, Rnw, Qmd or Word used to generate your report,

(ii) your R code (with CVmaster function in a separate R file),

(iii) a README.md file describing, in detail, how to reproduce your paper from scratch (assume researcher has access to the images).